{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3304fa00-9cbb-4613-9919-37dc781b1cae",
   "metadata": {},
   "source": [
    "---\n",
    "title: Oracle\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5e99e7-96c8-474f-bed9-b5ca0bbd61a1",
   "metadata": {},
   "source": [
    "Humans are not very good at producing random 0/1 sequences. We can guess their next number right more than 50% of the time !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e5b13f-cfa3-4e1f-87d7-da31fba1dd50",
   "metadata": {},
   "source": [
    "```{attention}\n",
    "- [ ] Describe problem we are trying to solve and the basic \"trick\" that we intend to use (learn based on the last few produced values)\n",
    "- [ ] Describe how to build the database (CSV file, 1 column, title \"Values\" first row, then 0 or 1, typically ~100 values)\n",
    "- [ ] Make custom Train + Test Dataset from database\n",
    "- [ ] model I/O interpretation : N numbers (0 or 1) -> normalized log p of 1 output\n",
    "- [ ] implement some baseline models as models with fixed parametes: pick H, pick tails, pick last one, pick opposite last one, restore the balance, etc. Test their performance on the test dataset.\n",
    "- [ ] Baseline accuracy vs best accuracy (via stats) -> algo score\n",
    "- [ ] make some additional data analysis to try to find some pattern in the dataset and guess some better performing \"fixed\" algorithm?\n",
    "- [ ] Try to learn the parameters of a linear model, see what the performance is.\n",
    "- [ ] Logistic regression?\n",
    "- [ ] Try with 1 or 2 hidden layers, tweak the params (sizes), etc, see how it goes.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c15dee88-eba8-48aa-b702-4e452bbbf690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9859462f-ba25-44ea-82f1-8660b24a9d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fecfb6-f100-4b9a-ad33-0cd6b6598f50",
   "metadata": {},
   "source": [
    "First, no options at construction (but later `transform`?), no support for tensors of indices (but required for later ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cf9b071-b788-41f4-8ea8-a3e8588b260f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0a84dd0-c350-428f-96f3-29effc75581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nota: ATM, a simple list would do. Can a simple list be given to a data loader? Test!\n",
    "# Do with and without transform and target_transform\n",
    "\n",
    "class HTDataset(Dataset):\n",
    "    def __init__(self, chunk_size=3, overlap=False, transform=None, target_transform=None): # size of the chunk used to predict\n",
    "        self.data = []\n",
    "        for csv_file in pathlib.Path(\"./db\").glob(\"*.csv\"):          \n",
    "            df = pd.read_csv(csv_file)\n",
    "            t = torch.tensor(df[\"Values\"])\n",
    "            if not overlap:\n",
    "                n = (len(t) - 1) // chunk_size\n",
    "                for i in range(0, n):\n",
    "                    j = chunk_size * i\n",
    "                    input = [x.item() for x in t[j:j+chunk_size]]\n",
    "                    output = t[j+chunk_size].item()\n",
    "                    self.data.append((input, output))\n",
    "            else:\n",
    "                n = len(t) - chunk_size\n",
    "                for i in range(0, n):\n",
    "                    input = [x.item() for x in t[i:i+chunk_size]]\n",
    "                    output = t[i+chunk_size].item()\n",
    "                    self.data.append((input, output))\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        # list all csv files in db dir\n",
    "        # for each file, \n",
    "        #    - use pandas to get the list or tensor of the stuff\n",
    "        #    - compute how many chunks (+1) we can extract from the values\n",
    "        #    - do it, add all the input/output stuff to the data list\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, i): # not vectorized\n",
    "        data = self.data[i]\n",
    "        input, output = data\n",
    "        if self.transform:\n",
    "            input = self.transform(input)\n",
    "        if self.target_transform:\n",
    "            output = self.target_transform(output)\n",
    "        data = input, output\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c49dfdd-6cfc-4c36-abfe-f5d73263a525",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = HTDataset(overlap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8c963e5-1b89-4d0b-b3ab-000ffa0965e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heads(input):\n",
    "    return 0\n",
    "\n",
    "def tails(input):\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "717b5fd3-7f71-4074-8264-781376c7539c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(algo, data):\n",
    "    successes = 0\n",
    "    failures = 0\n",
    "    for input, output in data:\n",
    "        if algo(input) == output:\n",
    "            successes += 1\n",
    "        else:\n",
    "            failures += 1\n",
    "    return successes / (successes + failures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b58d1baa-6c44-482d-86c3-225fef52cc65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5257731958762887"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(heads, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e905ebb8-9a12-43f7-9616-44226b7835ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4742268041237113"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(tails, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd7e800c-b3f1-4720-ab35-e6a99cd1732a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep(input):\n",
    "    return input[-1]\n",
    "\n",
    "def switch(input):\n",
    "    return 1 - input[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e645684e-77f6-4578-929d-6b4eef6f5bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31958762886597936"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(keep, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d68404b0-0bd5-4cc9-8f16-f6a2c9ec5050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6804123711340206"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(switch, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b018ae-6b76-4e35-82a1-1c0e95faa73a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c41b391b-aff6-4aae-9ba8-1faf7fbd750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14eac123-3962-46c8-9c52-2ae9f442eaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module, Linear\n",
    "\n",
    "def T(x):\n",
    "    return torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "class LR(Module):\n",
    "    def __init__(self, chunk_size=3):\n",
    "        super().__init__()\n",
    "        self.chunk_size = chunk_size\n",
    "        self.linear = Linear(chunk_size, 1)\n",
    "    def forward(self, input):\n",
    "        return self.linear(input)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "462ab213-3172-4afc-99a6-d9c3e8aadc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61e61e4a-8dee-4e1a-90aa-b87efcd5eb91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0975], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([0.0, 1.0, 0.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91cdb6d8-fb3f-467b-9c28-78a600359b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_wrapper(input):\n",
    "    model.eval()\n",
    "    input = torch.tensor(input, dtype=torch.float32)\n",
    "    out = model(input)\n",
    "    if out.item() >= 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d02e13d-8d1d-4c14-97cc-6c7e941a1f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wrapper([0, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2a45842-53ed-41ef-92a0-2065c2c010e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5257731958762887"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model_wrapper, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bedb504d-8f0f-4431-9142-52afaffcb679",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b92fb90-ed22-4b00-af4e-97d408bef603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y1, y2):\n",
    "    y1 = y1.squeeze()\n",
    "    y2 = y2.squeeze()\n",
    "    dy = y1-y2\n",
    "    return dy.abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b67f3abe-cfcb-4aad-b59a-580573e4b981",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72bf2f2d-f7e2-4e76-a0e7-efd465f228a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([input for input, _ in data], dtype=torch.float32) \n",
    "y = torch.tensor([output for _, output in data], dtype=torch.float32) \n",
    "\n",
    "def train(model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    \n",
    "    pred = model(X)\n",
    "    loss = loss_fn(pred, y)\n",
    "    #print(f\"loss: {loss.item()}\")\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    #print(list(model.named_parameters())[0])\n",
    "    optimizer.zero_grad()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "332214f6-3b1a-439a-911e-b32b2bea988a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('linear.weight', Parameter containing:\n",
      "tensor([[-0.4520, -0.0116, -0.5101]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
      "tensor([-0.0859], requires_grad=True))]\n",
      "loss: 1.0265779495239258\n",
      "accuracy: 0.5257731958762887\n",
      "------------------------------------------------------------\n",
      "[('linear.weight', Parameter containing:\n",
      "tensor([[-1.7782e-04,  2.2004e-01, -7.7982e-01]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
      "tensor([0.7798], requires_grad=True))]\n",
      "loss: 0.34006598591804504\n",
      "accuracy: 0.6804123711340206\n"
     ]
    }
   ],
   "source": [
    "pred = model(X)\n",
    "loss = loss_fn(pred, y)\n",
    "print(list(model.named_parameters()))\n",
    "print(\"loss:\", loss.item())\n",
    "print(\"accuracy:\", accuracy(model_wrapper, data))\n",
    "\n",
    "epochs = 10_000\n",
    "for t in range(epochs):\n",
    "    loss = train(model, loss_fn, optimizer)\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(list(model.named_parameters()))\n",
    "print(\"loss:\", loss)\n",
    "print(\"accuracy:\", accuracy(model_wrapper, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38345850-18e4-4696-8cd1-344e194e0ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escape the local minimum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96838bc7-b9d6-48b2-8c3a-283df374e26a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test a better strategy (switch)\n",
    "sd = model.state_dict()\n",
    "sd[\"linear.weight\"] = torch.tensor([[0.0, 0.0, -1.0]])\n",
    "sd[\"linear.bias\"] = torch.tensor([1.0])\n",
    "model.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5cb73f52-0767-4d50-b39a-2a93f11c8cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear.weight', tensor([[ 0.,  0., -1.]])),\n",
       "             ('linear.bias', tensor([1.]))])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79a5ee3b-c44f-41bd-991d-e5a219dbd644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('linear.weight', Parameter containing:\n",
      "tensor([[ 0.,  0., -1.]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
      "tensor([1.], requires_grad=True))]\n",
      "loss: 0.3195876181125641\n",
      "accuracy: 0.6804123711340206\n"
     ]
    }
   ],
   "source": [
    "pred = model(X)\n",
    "loss = loss_fn(pred, y)\n",
    "print(list(model.named_parameters()))\n",
    "print(\"loss:\", loss.item())\n",
    "print(\"accuracy:\", accuracy(model_wrapper, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ddf7a305-7b20-40a5-aecc-351fd0badd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mmm, there is an issue here, the loss appears to be larger (although the accuracy is better). \n",
    "# How come? Is it \"normal\" (and the MSE proxy that sucks) ? A mistake?\n",
    "# Actually, \"edging the bets\" and returning almost all the time the middle value 0.5\n",
    "# is probably strategic given that an error of 1.0 will be much more punished that 0.5\n",
    "# (4 x more costly!). So that kinda make sense ... we need to \"shape\" the loss function\n",
    "# much better. Would the L1 error work here? Mmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "06e855c1-82cb-47f2-9ca0-ec679d361a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y1, y2):\n",
    "    y1 = y1.squeeze()\n",
    "    y2 = y2.squeeze()\n",
    "    dy = y1-y2\n",
    "    return dy.abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7834956a-54ee-45ba-978c-b4cb7da5618c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8016de9-d282-49c8-aae9-38033f8c6a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc27f9ce-ae3e-49fd-aaa1-0d7794351ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('linear.weight', Parameter containing:\n",
      "tensor([[ 0.4350, -0.2285,  0.4200]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
      "tensor([-0.1959], requires_grad=True))]\n",
      "loss: 0.6889473795890808\n",
      "accuracy: 0.3917525773195876\n",
      "------------------------------------------------------------\n",
      "[('linear.weight', Parameter containing:\n",
      "tensor([[-9.2302e-04, -6.7032e-04, -9.9001e-01]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
      "tensor([1.0005], requires_grad=True))]\n",
      "loss: 0.3228142559528351\n",
      "accuracy: 0.6804123711340206\n"
     ]
    }
   ],
   "source": [
    "pred = model(X)\n",
    "loss = loss_fn(pred, y)\n",
    "print(list(model.named_parameters()))\n",
    "print(\"loss:\", loss.item())\n",
    "print(\"accuracy:\", accuracy(model_wrapper, data))\n",
    "\n",
    "epochs = 10_000\n",
    "for t in range(epochs):\n",
    "    loss = train(model, loss_fn, optimizer)\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(list(model.named_parameters()))\n",
    "print(\"loss:\", loss)\n",
    "print(\"accuracy:\", accuracy(model_wrapper, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "760f4055-4350-4e80-84aa-a6d34621be48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd = model.state_dict()\n",
    "sd[\"linear.weight\"] = torch.tensor([[0.0, 0.0, -1.0]])\n",
    "sd[\"linear.bias\"] = torch.tensor([1.0])\n",
    "model.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "238694cb-a59f-4b17-abd8-3af6b6454253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('linear.weight', Parameter containing:\n",
      "tensor([[ 0.,  0., -1.]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
      "tensor([1.], requires_grad=True))]\n",
      "loss: 0.3195876181125641\n",
      "accuracy: 0.6804123711340206\n"
     ]
    }
   ],
   "source": [
    "pred = model(X)\n",
    "loss = loss_fn(pred, y)\n",
    "print(list(model.named_parameters()))\n",
    "print(\"loss:\", loss.item())\n",
    "print(\"accuracy:\", accuracy(model_wrapper, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ece31a46-4725-4cfa-a8ff-55e7c4e6d910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mmm this is also a poor proxy for accuracy (even if it seems less shitty?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489a24c4-4f4a-48be-935b-9d0624be0f19",
   "metadata": {},
   "source": [
    "Here we could actually display all cases (inputs: 2**3 = 8) and see what the likelist outcome is in each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b03535c1-f44a-4876-940f-701276b24e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also, are things different if we start with overlaping data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b61da07c-025c-4d6d-b1b1-6e5c63f518a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0, 0, 0): 1,\n",
       " (0, 0, 0, 1): 6,\n",
       " (0, 0, 1, 0): 5,\n",
       " (0, 0, 1, 1): 5,\n",
       " (0, 1, 0, 0): 8,\n",
       " (0, 1, 0, 1): 15,\n",
       " (0, 1, 1, 0): 8,\n",
       " (0, 1, 1, 1): 2,\n",
       " (1, 0, 0, 0): 7,\n",
       " (1, 0, 0, 1): 4,\n",
       " (1, 0, 1, 0): 18,\n",
       " (1, 0, 1, 1): 5,\n",
       " (1, 1, 0, 0): 2,\n",
       " (1, 1, 0, 1): 8,\n",
       " (1, 1, 1, 0): 2,\n",
       " (1, 1, 1, 1): 1}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = {}\n",
    "for X, y in data:\n",
    "    d = tuple(X) + (y,)\n",
    "    if d not in l:\n",
    "        l[d] = 0\n",
    "    l[d] += 1\n",
    "r = sorted(l.items())\n",
    "l = dict(r)\n",
    "l       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5cc7127b-c4d7-48b9-a637-233fc517eb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = HTDataset(overlap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c63c91eb-53c1-425d-af43-71b239bf87cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5257731958762887"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(heads, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ccb0c99e-5fbe-4313-b9e2-768abcf7c778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0, 0, 0): 1,\n",
       " (0, 0, 0, 1): 6,\n",
       " (0, 0, 1, 0): 5,\n",
       " (0, 0, 1, 1): 5,\n",
       " (0, 1, 0, 0): 8,\n",
       " (0, 1, 0, 1): 15,\n",
       " (0, 1, 1, 0): 8,\n",
       " (0, 1, 1, 1): 2,\n",
       " (1, 0, 0, 0): 7,\n",
       " (1, 0, 0, 1): 4,\n",
       " (1, 0, 1, 0): 18,\n",
       " (1, 0, 1, 1): 5,\n",
       " (1, 1, 0, 0): 2,\n",
       " (1, 1, 0, 1): 8,\n",
       " (1, 1, 1, 0): 2,\n",
       " (1, 1, 1, 1): 1}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = {}\n",
    "for X, y in data:\n",
    "    d = tuple(X) + (y,)\n",
    "    if d not in l:\n",
    "        l[d] = 0\n",
    "    l[d] += 1\n",
    "r = sorted(l.items())\n",
    "l = dict(r)\n",
    "l       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eaeaed2f-3ece-496a-9647-8846fadb84f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([input for input, _ in data], dtype=torch.float32) \n",
    "y = torch.tensor([output for _, output in data], dtype=torch.float32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f42cf8cd-231a-49d0-a431-accbe31a77cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('linear.weight', Parameter containing:\n",
      "tensor([[ 0.,  0., -1.]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
      "tensor([1.], requires_grad=True))]\n",
      "loss: 0.3195876181125641\n",
      "accuracy: 0.6804123711340206\n",
      "------------------------------------------------------------\n",
      "[('linear.weight', Parameter containing:\n",
      "tensor([[-0.0012,  0.0024, -0.9935]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
      "tensor([1.0015], requires_grad=True))]\n",
      "loss: 0.32053428888320923\n",
      "accuracy: 0.6804123711340206\n"
     ]
    }
   ],
   "source": [
    "pred = model(X)\n",
    "loss = loss_fn(pred, y)\n",
    "print(list(model.named_parameters()))\n",
    "print(\"loss:\", loss.item())\n",
    "print(\"accuracy:\", accuracy(model_wrapper, data))\n",
    "\n",
    "epochs = 10_000\n",
    "for t in range(epochs):\n",
    "    loss = train(model, loss_fn, optimizer)\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(list(model.named_parameters()))\n",
    "print(\"loss:\", loss)\n",
    "print(\"accuracy:\", accuracy(model_wrapper, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d269ea-d876-4fe8-9251-2e80eef123ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "441b215f-6594-4a99-af70-1a6490ee3c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 1\n",
    "\n",
    "data = HTDataset(chunk_size=chunk_size, overlap=False)\n",
    "\n",
    "X = torch.tensor([input for input, _ in data], dtype=torch.float32) \n",
    "y = torch.tensor([output for _, output in data], dtype=torch.float32) \n",
    "\n",
    "model = LR(chunk_size=chunk_size)\n",
    "\n",
    "def loss_fn(y1, y2):\n",
    "    y1 = y1.squeeze()\n",
    "    y2 = y2.squeeze()\n",
    "    dy = y1-y2\n",
    "    return dy.abs().mean()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "\n",
    "def train(model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    \n",
    "    pred = model(X)\n",
    "    loss = loss_fn(pred, y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f17b58dc-a365-4e64-b48d-28825fdcf335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('linear.weight', Parameter containing:\n",
      "tensor([[0.6998]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
      "tensor([0.2212], requires_grad=True))]\n",
      "loss: 0.6287241578102112\n",
      "accuracy: 0.32323232323232326\n",
      "------------------------------------------------------------\n",
      "[('linear.weight', Parameter containing:\n",
      "tensor([[-0.9950]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
      "tensor([0.9964], requires_grad=True))]\n",
      "loss: 0.32752057909965515\n",
      "accuracy: 0.6767676767676768\n"
     ]
    }
   ],
   "source": [
    "pred = model(X)\n",
    "loss = loss_fn(pred, y)\n",
    "print(list(model.named_parameters()))\n",
    "print(\"loss:\", loss.item())\n",
    "print(\"accuracy:\", accuracy(model_wrapper, data))\n",
    "\n",
    "epochs = 10_000\n",
    "for t in range(epochs):\n",
    "    loss = train(model, loss_fn, optimizer)\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(list(model.named_parameters()))\n",
    "print(\"loss:\", loss)\n",
    "print(\"accuracy:\", accuracy(model_wrapper, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "608ad951-3273-48df-a145-fcb7653dad66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6767676767676768"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(switch, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8b7cdc11-5912-4ff0-adc9-7490a7154e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5353535353535354"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(heads, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b6151b1d-5a7e-4721-9f81-1f735859ea79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wrapper([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "17ff2726-e113-4960-ae77-09e33ef86e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wrapper([0]) # actually heads ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c730620e-1e70-4934-a58b-1020d9b0f81e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LR(\n",
       "  (linear): Linear(in_features=1, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0686b03f-cfdf-4b2a-80c4-5d656568399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = LR(chunk_size=1)\n",
    "with torch.no_grad():\n",
    "    model2.linear.weight[0,0] = -1.0\n",
    "    model2.linear.bias[0] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ee1bbde2-d4b7-485b-ab25-fedacdeff92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model3 = LR(chunk_size=1)\n",
    "model3.eval()\n",
    "with torch.no_grad():\n",
    "    model3.linear.weight[0,0] = -0.02\n",
    "    model3.linear.bias[0] = 0.51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f8c3f5b0-1668-469a-ac57-8c2b0ed53194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>in</th>\n",
       "      <th>out</th>\n",
       "      <th>pred 1</th>\n",
       "      <th>pred 2</th>\n",
       "      <th>pred 3</th>\n",
       "      <th>err 1</th>\n",
       "      <th>err 2</th>\n",
       "      <th>err 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.992805</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.992805</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.997229</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.997229</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.997229</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.992805</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.992805</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.997229</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.997229</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.997229</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.992805</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.997229</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.997229</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.992805</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.992805</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.992805</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.997229</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.992805</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.992805</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.997229</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.997229</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.992805</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.997229</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.992805</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.992805</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.992805</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.997229</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.992805</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.992805</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.992805</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.992805</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.996396</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.992805</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    in  out    pred 1  pred 2  pred 3     err 1  err 2   err 3\n",
       "0    1    0  0.001386     0.0    0.49  0.000002    0.0  0.2401\n",
       "1    0    0  0.996396     1.0    0.51  0.992805    1.0  0.2601\n",
       "2    0    0  0.996396     1.0    0.51  0.992805    1.0  0.2601\n",
       "3    0    1  0.996396     1.0    0.51  0.000013    0.0  0.2401\n",
       "4    1    1  0.001386     0.0    0.49  0.997229    1.0  0.2601\n",
       "5    1    0  0.001386     0.0    0.49  0.000002    0.0  0.2401\n",
       "6    0    1  0.996396     1.0    0.51  0.000013    0.0  0.2401\n",
       "7    1    0  0.001386     0.0    0.49  0.000002    0.0  0.2401\n",
       "8    0    1  0.996396     1.0    0.51  0.000013    0.0  0.2401\n",
       "9    1    1  0.001386     0.0    0.49  0.997229    1.0  0.2601\n",
       "10   1    1  0.001386     0.0    0.49  0.997229    1.0  0.2601\n",
       "11   1    0  0.001386     0.0    0.49  0.000002    0.0  0.2401\n",
       "12   0    1  0.996396     1.0    0.51  0.000013    0.0  0.2401\n",
       "13   1    0  0.001386     0.0    0.49  0.000002    0.0  0.2401\n",
       "14   0    0  0.996396     1.0    0.51  0.992805    1.0  0.2601\n",
       "15   0    0  0.996396     1.0    0.51  0.992805    1.0  0.2601\n",
       "16   0    1  0.996396     1.0    0.51  0.000013    0.0  0.2401\n",
       "17   1    1  0.001386     0.0    0.49  0.997229    1.0  0.2601\n",
       "18   1    1  0.001386     0.0    0.49  0.997229    1.0  0.2601\n",
       "19   1    1  0.001386     0.0    0.49  0.997229    1.0  0.2601\n",
       "20   1    0  0.001386     0.0    0.49  0.000002    0.0  0.2401\n",
       "21   0    1  0.996396     1.0    0.51  0.000013    0.0  0.2401\n",
       "22   1    0  0.001386     0.0    0.49  0.000002    0.0  0.2401\n",
       "23   0    1  0.996396     1.0    0.51  0.000013    0.0  0.2401\n",
       "24   1    0  0.001386     0.0    0.49  0.000002    0.0  0.2401\n",
       "25   0    0  0.996396     1.0    0.51  0.992805    1.0  0.2601\n",
       "26   0    1  0.996396     1.0    0.51  0.000013    0.0  0.2401\n",
       "27   1    1  0.001386     0.0    0.49  0.997229    1.0  0.2601\n",
       "28   1    0  0.001386     0.0    0.49  0.000002    0.0  0.2401\n",
       "29   0    1  0.996396     1.0    0.51  0.000013    0.0  0.2401\n",
       "30   1    1  0.001386     0.0    0.49  0.997229    1.0  0.2601\n",
       "31   1    0  0.001386     0.0    0.49  0.000002    0.0  0.2401\n",
       "32   0    0  0.996396     1.0    0.51  0.992805    1.0  0.2601\n",
       "33   0    1  0.996396     1.0    0.51  0.000013    0.0  0.2401\n",
       "34   1    0  0.001386     0.0    0.49  0.000002    0.0  0.2401\n",
       "35   0    1  0.996396     1.0    0.51  0.000013    0.0  0.2401\n",
       "36   1    0  0.001386     0.0    0.49  0.000002    0.0  0.2401\n",
       "37   0    0  0.996396     1.0    0.51  0.992805    1.0  0.2601\n",
       "38   0    0  0.996396     1.0    0.51  0.992805    1.0  0.2601\n",
       "39   0    1  0.996396     1.0    0.51  0.000013    0.0  0.2401\n",
       "40   1    0  0.001386     0.0    0.49  0.000002    0.0  0.2401\n",
       "41   0    1  0.996396     1.0    0.51  0.000013    0.0  0.2401\n",
       "42   1    1  0.001386     0.0    0.49  0.997229    1.0  0.2601\n",
       "43   1    0  0.001386     0.0    0.49  0.000002    0.0  0.2401\n",
       "44   0    0  0.996396     1.0    0.51  0.992805    1.0  0.2601\n",
       "45   0    0  0.996396     1.0    0.51  0.992805    1.0  0.2601\n",
       "46   0    1  0.996396     1.0    0.51  0.000013    0.0  0.2401\n",
       "47   1    1  0.001386     0.0    0.49  0.997229    1.0  0.2601\n",
       "48   1    0  0.001386     0.0    0.49  0.000002    0.0  0.2401\n",
       "49   0    1  0.996396     1.0    0.51  0.000013    0.0  0.2401\n",
       "50   1    0  0.001386     0.0    0.49  0.000002    0.0  0.2401\n",
       "51   0    1  0.996396     1.0    0.51  0.000013    0.0  0.2401\n",
       "52   1    0  0.001386     0.0    0.49  0.000002    0.0  0.2401\n",
       "53   0    1  0.996396     1.0    0.51  0.000013    0.0  0.2401\n",
       "54   1    1  0.001386     0.0    0.49  0.997229    1.0  0.2601\n",
       "55   1    0  0.001386     0.0    0.49  0.000002    0.0  0.2401\n",
       "56   0    1  0.996396     1.0    0.51  0.000013    0.0  0.2401\n",
       "57   1    0  0.001386     0.0    0.49  0.000002    0.0  0.2401\n",
       "58   0    1  0.996396     1.0    0.51  0.000013    0.0  0.2401\n",
       "59   1    0  0.001386     0.0    0.49  0.000002    0.0  0.2401\n",
       "60   0    0  0.996396     1.0    0.51  0.992805    1.0  0.2601\n",
       "61   0    1  0.996396     1.0    0.51  0.000013    0.0  0.2401\n",
       "62   1    1  0.001386     0.0    0.49  0.997229    1.0  0.2601\n",
       "63   1    0  0.001386     0.0    0.49  0.000002    0.0  0.2401\n",
       "64   0    1  0.996396     1.0    0.51  0.000013    0.0  0.2401\n",
       "65   1    0  0.001386     0.0    0.49  0.000002    0.0  0.2401\n",
       "66   0    0  0.996396     1.0    0.51  0.992805    1.0  0.2601\n",
       "67   0    0  0.996396     1.0    0.51  0.992805    1.0  0.2601\n",
       "68   0    1  0.996396     1.0    0.51  0.000013    0.0  0.2401\n",
       "69   1    0  0.001386     0.0    0.49  0.000002    0.0  0.2401\n",
       "70   0    1  0.996396     1.0    0.51  0.000013    0.0  0.2401\n",
       "71   1    0  0.001386     0.0    0.49  0.000002    0.0  0.2401\n",
       "72   0    0  0.996396     1.0    0.51  0.992805    1.0  0.2601\n",
       "73   0    1  0.996396     1.0    0.51  0.000013    0.0  0.2401\n",
       "74   1    0  0.001386     0.0    0.49  0.000002    0.0  0.2401\n",
       "75   0    1  0.996396     1.0    0.51  0.000013    0.0  0.2401\n",
       "76   1    0  0.001386     0.0    0.49  0.000002    0.0  0.2401\n",
       "77   0    1  0.996396     1.0    0.51  0.000013    0.0  0.2401\n",
       "78   1    0  0.001386     0.0    0.49  0.000002    0.0  0.2401\n",
       "79   0    1  0.996396     1.0    0.51  0.000013    0.0  0.2401\n",
       "80   1    0  0.001386     0.0    0.49  0.000002    0.0  0.2401\n",
       "81   0    1  0.996396     1.0    0.51  0.000013    0.0  0.2401\n",
       "82   1    0  0.001386     0.0    0.49  0.000002    0.0  0.2401\n",
       "83   0    1  0.996396     1.0    0.51  0.000013    0.0  0.2401\n",
       "84   1    0  0.001386     0.0    0.49  0.000002    0.0  0.2401\n",
       "85   0    1  0.996396     1.0    0.51  0.000013    0.0  0.2401\n",
       "86   1    1  0.001386     0.0    0.49  0.997229    1.0  0.2601\n",
       "87   1    0  0.001386     0.0    0.49  0.000002    0.0  0.2401\n",
       "88   0    1  0.996396     1.0    0.51  0.000013    0.0  0.2401\n",
       "89   1    0  0.001386     0.0    0.49  0.000002    0.0  0.2401\n",
       "90   0    0  0.996396     1.0    0.51  0.992805    1.0  0.2601\n",
       "91   0    0  0.996396     1.0    0.51  0.992805    1.0  0.2601\n",
       "92   0    0  0.996396     1.0    0.51  0.992805    1.0  0.2601\n",
       "93   0    1  0.996396     1.0    0.51  0.000013    0.0  0.2401\n",
       "94   1    0  0.001386     0.0    0.49  0.000002    0.0  0.2401\n",
       "95   0    1  0.996396     1.0    0.51  0.000013    0.0  0.2401\n",
       "96   1    0  0.001386     0.0    0.49  0.000002    0.0  0.2401\n",
       "97   0    0  0.996396     1.0    0.51  0.992805    1.0  0.2601\n",
       "98   0    0  0.996396     1.0    0.51  0.992805    1.0  0.2601"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "for X, y in data:\n",
    "    out1 = model(torch.tensor(X, dtype=torch.float32))\n",
    "    out1 = out1.item()\n",
    "    out2 = model2(torch.tensor(X, dtype=torch.float32))\n",
    "    out2 = out2.item()\n",
    "    out3 = model3(torch.tensor(X, dtype=torch.float32))\n",
    "    out3 = out3.item()\n",
    "    rows.append({\"in\": X[0], \n",
    "                 \"out\": y, \n",
    "                 \"pred 1\": out1, \"pred 2\": out2, \"pred 3\": out3,\n",
    "                 \"err 1\": (y-out1)**2, \"err 2\": (y-out2)**2, \"err 3\": (y-out3)**2,\n",
    "                })\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "df = pd.DataFrame(rows)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bff40931-505d-4a2c-84c4-e854b4ecd7be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3214926769293237, 0.32323232323232326, 0.2465646436067545)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"err 1\"].mean(), df[\"err 2\"].mean(), df[\"err 3\"].mean()\n",
    "# So we can have a model with a better accuracy (switch) with an cost that is actually lower\n",
    "# So why is our algorithm not able to find it? Try to initialize in the vicinity and see what happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d9145c49-1068-46d3-82f4-d7bd28325944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_wrapper_ho(model):\n",
    "    def _model_wrapper(input):\n",
    "        model.eval()\n",
    "        input = torch.tensor(input, dtype=torch.float32)\n",
    "        out = model(input)\n",
    "        if out.item() >= 0.5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    return _model_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b9d3b46d-f4e7-4e1a-a101-4e78289ed9de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6767676767676768"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model_wrapper_ho(model), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1486c2a3-d1a7-4347-8e15-b9ce61f7d6cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6767676767676768"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model_wrapper_ho(model2), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bce1e45e-65bd-49c0-adae-3ded6d6cf321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6767676767676768"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model_wrapper_ho(model3), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2f9e83c1-42cf-4745-9f69-07eb0e668d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 3\n",
    "\n",
    "data = HTDataset(chunk_size=chunk_size, overlap=False)\n",
    "\n",
    "X = torch.tensor([input for input, _ in data], dtype=torch.float32) \n",
    "y = torch.tensor([output for _, output in data], dtype=torch.float32) \n",
    "\n",
    "model = LR(chunk_size=chunk_size)\n",
    "with torch.no_grad():\n",
    "    model.linear.weight[0,0] = -0.02\n",
    "    model.linear.bias[0] = 0.51\n",
    "\n",
    "def loss_fn(y1, y2):\n",
    "    y1 = y1.squeeze()\n",
    "    y2 = y2.squeeze()\n",
    "    dy = y1-y2\n",
    "    return (dy*dy).mean()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train(model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    \n",
    "    pred = model(X)\n",
    "    loss = loss_fn(pred, y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1c26ca5d-cb15-4b5a-8c77-5a5dd3092063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('linear.weight', Parameter containing:\n",
      "tensor([[-0.0200, -0.5234,  0.2696]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
      "tensor([0.5100], requires_grad=True))]\n",
      "loss: 0.4935130476951599\n",
      "accuracy: 0.42424242424242425\n",
      "------------------------------------------------------------\n",
      "[('linear.weight', Parameter containing:\n",
      "tensor([[-0.4428,  0.0664, -0.2080]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
      "tensor([0.7670], requires_grad=True))]\n",
      "loss: 0.1695430725812912\n",
      "accuracy: 0.7575757575757576\n"
     ]
    }
   ],
   "source": [
    "pred = model(X)\n",
    "loss = loss_fn(pred, y)\n",
    "print(list(model.named_parameters()))\n",
    "print(\"loss:\", loss.item())\n",
    "print(\"accuracy:\", accuracy(model_wrapper, data))\n",
    "\n",
    "epochs = 10_000\n",
    "for t in range(epochs):\n",
    "    loss = train(model, loss_fn, optimizer)\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(list(model.named_parameters()))\n",
    "print(\"loss:\", loss)\n",
    "print(\"accuracy:\", accuracy(model_wrapper, data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd98c85d-d7b6-4513-8e6c-851a295cebb8",
   "metadata": {},
   "source": [
    "## Logistic (differential score) output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9816087b-5eb2-46d1-98e0-9ddf9591cf4b",
   "metadata": {},
   "source": [
    "Now we interpret the model output $\\ell$, the difference between score of 0 and score of 1, that is\n",
    "\n",
    "$$\n",
    "\\mathrm{\\ell} = \\log p +c - \\log (1 -p) -c = \\log \\frac{p}{1-p}\n",
    "$$                                                            \n",
    "\n",
    "where $p$ is the probability of having 0. Consequently, \n",
    "\n",
    "$$\n",
    "\\exp \\ell = \\frac{p}{1-p}\n",
    "$$\n",
    "\n",
    "$$\n",
    "(1 - p)\\exp \\ell - p = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "(1+\\exp \\ell) p = \\exp \\ell\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "p = \\frac{\\exp \\ell}{1 + \\exp \\ell} = \\frac{1}{1 + \\exp(-\\ell)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5614f1-3a68-4a7a-9763-50c877185feb",
   "metadata": {},
   "source": [
    "Note that in this context we have:\n",
    "$$\n",
    "-\\log p =  \\log (1+\\exp(-s))\n",
    "$$\n",
    "and\n",
    "$$\n",
    "-\\log (1-p) = \\log (1+\\exp s).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0168818a-5a33-489a-84a2-1c51b9ccdc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(Module):\n",
    "    def __init__(self, chunk_size=3):\n",
    "        super().__init__()\n",
    "        self.chunk_size = chunk_size\n",
    "        self.linear = Linear(chunk_size, 1)\n",
    "    def forward(self, input):\n",
    "        return self.linear(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "49a2fcba-a735-4e4d-a83a-390ef5a3353f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(pred, y):\n",
    "    pred = pred.squeeze()\n",
    "    y = y.squeeze()\n",
    "    cross_entropy_terms = y * torch.log(1.0 + torch.exp(-pred)) + (1 - y) * torch.log(1.0 + torch.exp(pred))\n",
    "    return cross_entropy_terms.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6ff08759-cef0-4551-8092-872adc0a4b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 1\n",
    "data = HTDataset(overlap=False, chunk_size=chunk_size)\n",
    "X = torch.tensor([input for input, _ in data], dtype=torch.float32) \n",
    "y = torch.tensor([output for _, output in data], dtype=torch.float32) \n",
    "model = Model(chunk_size=chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eb802816-d0af-45af-b203-5aa104ceaea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6985, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "pred = model(X)\n",
    "loss_fn(pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "76a66af7-8a07-43bb-a2b9-bc1d0466ba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train(model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    \n",
    "    pred = model(X)\n",
    "    loss = loss_fn(pred, y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5fd18edb-53e5-4365-99bf-ef6860e60dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_model_wrapper_ho(model):\n",
    "    def _model_wrapper(input):\n",
    "        model.eval()\n",
    "        input = torch.tensor(input, dtype=torch.float32)\n",
    "        out = model(input)\n",
    "        if out.item() >= 0.0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    return _model_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "aa2985f9-354c-4ff6-b272-739639d0e7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('linear.weight', Parameter containing:\n",
      "tensor([[0.0451]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
      "tensor([0.0107], requires_grad=True))]\n",
      "loss: 0.6985028982162476\n",
      "accuracy: 0.46464646464646464\n",
      "------------------------------------------------------------\n",
      "[('linear.weight', Parameter containing:\n",
      "tensor([[-1.4960]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
      "tensor([0.5425], requires_grad=True))]\n",
      "loss: 0.6247754693031311\n",
      "accuracy: 0.6767676767676768\n"
     ]
    }
   ],
   "source": [
    "pred = model(X)\n",
    "loss = loss_fn(pred, y)\n",
    "print(list(model.named_parameters()))\n",
    "print(\"loss:\", loss.item())\n",
    "print(\"accuracy:\", accuracy(logistic_model_wrapper_ho(model), data))\n",
    "\n",
    "epochs = 100_000\n",
    "for t in range(epochs):\n",
    "    loss = train(model, loss_fn, optimizer)\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(list(model.named_parameters()))\n",
    "print(\"loss:\", loss)\n",
    "print(\"accuracy:\", accuracy(logistic_model_wrapper_ho(model), data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2d6211bf-23d9-47e8-8a4c-cc196d8ca923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5425], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "model(torch.tensor([0.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2d7a1d0b-edb7-492a-bcb7-fd998e839063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9535], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4a831237-80b7-4239-8d3c-4c71835c116e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5425],\n",
       "        [-0.9535]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([[0.0], [1.0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d6994fad-9bbb-4bc7-a455-8785a507f01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 3\n",
    "data = HTDataset(overlap=False, chunk_size=chunk_size)\n",
    "X = torch.tensor([input for input, _ in data], dtype=torch.float32) \n",
    "y = torch.tensor([output for _, output in data], dtype=torch.float32) \n",
    "model = Model(chunk_size=chunk_size)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train(model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    \n",
    "    pred = model(X)\n",
    "    loss = loss_fn(pred, y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "009e76ba-98dd-48f0-b39b-19a220692b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('linear.weight', Parameter containing:\n",
      "tensor([[-2.1248,  0.2879, -1.1662]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
      "tensor([1.3609], requires_grad=True))]\n",
      "loss: 0.5157646536827087\n",
      "accuracy: 0.7575757575757576\n",
      "------------------------------------------------------------\n",
      "[('linear.weight', Parameter containing:\n",
      "tensor([[-2.1486,  0.2489, -1.2050]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
      "tensor([1.4113], requires_grad=True))]\n",
      "loss: 0.5156917572021484\n",
      "accuracy: 0.7575757575757576\n"
     ]
    }
   ],
   "source": [
    "pred = model(X)\n",
    "loss = loss_fn(pred, y)\n",
    "print(list(model.named_parameters()))\n",
    "print(\"loss:\", loss.item())\n",
    "print(\"accuracy:\", accuracy(logistic_model_wrapper_ho(model), data))\n",
    "\n",
    "epochs = 100_000\n",
    "for t in range(epochs):\n",
    "    loss = train(model, loss_fn, optimizer)\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(list(model.named_parameters()))\n",
    "print(\"loss:\", loss)\n",
    "print(\"accuracy:\", accuracy(logistic_model_wrapper_ho(model), data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f65700c7-3870-4de6-ad0f-f97a56f08cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(Module):\n",
    "    def __init__(self, chunk_size=3, n=8):\n",
    "        super().__init__()\n",
    "        self.chunk_size = chunk_size\n",
    "        self.linear1 = Linear(chunk_size, n)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.linear2 = Linear(n, 1)\n",
    "    def forward(self, input):\n",
    "        x = input\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f22856d5-d686-40e6-bad9-9b75fc0bf564",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 3\n",
    "n = 8\n",
    "\n",
    "data = HTDataset(overlap=False, chunk_size=chunk_size)\n",
    "X = torch.tensor([input for input, _ in data], dtype=torch.float32) \n",
    "y = torch.tensor([output for _, output in data], dtype=torch.float32) \n",
    "model = NN(chunk_size=chunk_size, n=n)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train(model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    \n",
    "    pred = model(X)\n",
    "    loss = loss_fn(pred, y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ba368a15-74e9-4082-8d7a-4546787e097a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('linear1.weight', Parameter containing:\n",
      "tensor([[-0.2992, -0.1142, -0.0998],\n",
      "        [-0.0440, -0.3238,  0.2676],\n",
      "        [ 0.5563, -0.0875,  0.1623],\n",
      "        [-0.4545, -0.5577, -0.0578],\n",
      "        [-0.0668,  0.0722,  0.4203],\n",
      "        [ 0.2073, -0.2592,  0.4398],\n",
      "        [ 0.2173, -0.5751,  0.4275],\n",
      "        [ 0.0763,  0.3588, -0.4070]], requires_grad=True)), ('linear1.bias', Parameter containing:\n",
      "tensor([-0.5157, -0.1978,  0.5552,  0.1567, -0.4367,  0.2838, -0.1767,  0.0427],\n",
      "       requires_grad=True)), ('linear2.weight', Parameter containing:\n",
      "tensor([[-0.3499,  0.3290,  0.0847,  0.0150, -0.0962,  0.0499, -0.1699, -0.0081]],\n",
      "       requires_grad=True)), ('linear2.bias', Parameter containing:\n",
      "tensor([0.3352], requires_grad=True))]\n",
      "loss: 0.7277809977531433\n",
      "accuracy: 0.48484848484848486\n",
      "------------------------------------------------------------\n",
      "[('linear1.weight', Parameter containing:\n",
      "tensor([[-0.2992, -0.1142, -0.0998],\n",
      "        [-0.1539, -0.9003,  0.9004],\n",
      "        [ 1.1779,  0.1940,  0.4933],\n",
      "        [-0.4568, -0.5577,  0.2433],\n",
      "        [ 0.1094,  0.5234,  0.5176],\n",
      "        [ 0.7297,  0.1336,  0.7116],\n",
      "        [ 0.6739, -0.2957,  0.3245],\n",
      "        [ 0.0569,  1.0935, -1.0936]], requires_grad=True)), ('linear1.bias', Parameter containing:\n",
      "tensor([-5.1567e-01,  3.5284e-06,  2.1095e-01,  2.5291e-01, -5.1761e-01,\n",
      "         1.8564e-01, -3.2450e-01,  5.8559e-06], requires_grad=True)), ('linear2.weight', Parameter containing:\n",
      "tensor([[-0.3499,  1.2392, -1.0339,  0.3124, -0.6738, -0.8427, -0.4345,  1.4467]],\n",
      "       requires_grad=True)), ('linear2.bias', Parameter containing:\n",
      "tensor([0.9735], requires_grad=True))]\n",
      "loss: 0.4610528349876404\n",
      "accuracy: 0.7878787878787878\n"
     ]
    }
   ],
   "source": [
    "pred = model(X)\n",
    "loss = loss_fn(pred, y)\n",
    "print(list(model.named_parameters()))\n",
    "print(\"loss:\", loss.item())\n",
    "print(\"accuracy:\", accuracy(logistic_model_wrapper_ho(model), data))\n",
    "\n",
    "epochs = 100_000\n",
    "for t in range(epochs):\n",
    "    loss = train(model, loss_fn, optimizer)\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(list(model.named_parameters()))\n",
    "print(\"loss:\", loss)\n",
    "print(\"accuracy:\", accuracy(logistic_model_wrapper_ho(model), data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05fdde8-3fee-4f63-b0ef-048e1fabe413",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
